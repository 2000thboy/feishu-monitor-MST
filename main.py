import os
import re
import requests
import json
import time
from datetime import datetime, timedelta
from collections import defaultdict

# ================= é…ç½®åŒº =================
APP_ID = "cli_a9a427abc73a1bc7"
APP_SECRET = "xza3K8d65ks5DcN9DG1P7dTAXKNYLz5E"

# è¡¨æ ¼ Token
SPREADSHEET_TOKEN = "Y7sEsZsjrhcQyvt0U7HcyqGPnNh"

# ä¼ä¸šå¾®ä¿¡ Webhook
WECOM_WEBHOOK = "https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=9f59729a-0140-4044-88a2-026996d894bb"

# å­è¡¨ ID
TARGET_SHEET_IDS = ["Z7k4T5"]

class MonitorBot:
    def __init__(self):
        self.token = ""
        self.sheet_names = {} 
        self.scanned_list = []
        self.error_count = 0    

    def get_tenant_access_token(self):
        url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
        try:
            resp = requests.post(url, json={"app_id": APP_ID, "app_secret": APP_SECRET}).json()
            if resp.get("code") == 0:
                self.token = resp.get("tenant_access_token")
            else:
                print(f"âŒ é£ä¹¦APIè®¤è¯å¤±è´¥: {resp}")
        except Exception as e:
            print(f"âŒ ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")

    def load_all_sheet_names(self):
        if not self.token: self.get_tenant_access_token()
        url = f"https://open.feishu.cn/open-apis/sheets/v3/spreadsheets/{SPREADSHEET_TOKEN}/sheets/query"
        headers = {"Authorization": f"Bearer {self.token}"}
        resp = requests.get(url, headers=headers).json()
        if resp.get("code") == 0:
            sheets = resp.get("data", {}).get("sheets", [])
            for sheet in sheets:
                self.sheet_names[sheet.get("sheet_id")] = sheet.get("title", "æœªå‘½å")
            print(f"ğŸ“š è¡¨æ ¼ååŠ è½½å®Œæ¯•")

    def clean_text(self, cell_data):
        if cell_data is None: return ""
        if isinstance(cell_data, str): return cell_data
        
        def extract_segment(seg):
            if not isinstance(seg, dict): return str(seg)
            if 'fileToken' in seg or 'image_key' in seg or seg.get('type') in ['embed-image', 'file', 'mention']:
                return ""
            return seg.get('text', "")

        if isinstance(cell_data, list):
            text_list = []
            for segment in cell_data:
                text_list.append(extract_segment(segment))
            return "".join(text_list)
            
        if isinstance(cell_data, dict):
            return extract_segment(cell_data)

        return str(cell_data)

    def is_safe_content(self, text):
        safe_words = [
            "é€šè¿‡", "å®Œæˆ", "æ— éœ€", "pass", "ok", "done", 
            "æäº¤ä¸‹æ¸¸", "å·²æäº¤ä¸‹æ¸¸", "äº¤ä¸‹æ¸¸", "-", "/"
        ]
        text_lower = text.lower()
        # å¦‚æœæ˜¯å•ä¸ª - æˆ– / è§†ä¸ºæ— éœ€å¡«å†™ï¼Œä¸ç®—ç©ºç™½
        if text.strip() in ["-", "/"]: return True
        return any(w in text_lower for w in safe_words)

    def is_noise(self, text):
        t = text.strip().lower()
        if not t: return True # ç©ºæ–‡æœ¬ç”±ä¸»é€»è¾‘å¤„ç†ï¼Œæ­¤å¤„ä»…è¿‡æ»¤çº¯ç¬¦å·å™ªéŸ³
        if t in ["\\", "."]: return True
        return False

    def has_chinese(self, text):
        return bool(re.search(r'[\u4e00-\u9fa5]', text))

    def get_column_letter(self, col_idx):
        if col_idx < 26: return chr(65 + col_idx)
        else: return 'A' + chr(65 + (col_idx - 26))

    def find_shot_number(self, row):
        scan_limit = min(len(row), 5)
        shot_pattern = re.compile(r'(?i)[a-z]+[-_]?\d+') 
        for i in range(scan_limit):
            text = self.clean_text(row[i]).strip()
            if not text: continue
            if text in ["é•œå·", "é•œå¤´", "åºå·"]: continue 
            if shot_pattern.search(text):
                return text
        return None

    def find_stage_name_dynamic(self, col_idx, header1, header2):
        skip_keywords = ["åé¦ˆ", "è¯´æ˜", "éœ€æ±‚", "çŠ¶æ€", "CK", "Time", "å½“å‰", "è¿›åº¦", "ç´ æ"]
        for j in range(col_idx, -1, -1):
            h1 = self.clean_text(header1[j] if j < len(header1) else "").strip()
            if not h1: continue
            if any(k in h1 for k in skip_keywords): continue
            return h1
        return "æœªçŸ¥ç¯èŠ‚"

    # âœ… æ ¸å¿ƒä¿®æ”¹ï¼šåŒæ—¶æ£€æµ‹æ—¥æœŸå’Œç©ºç™½
    def scan_row_full(self, row, now, header1, header2):
        total_cols = len(row)
        issues = []
        
        for i in range(total_cols):
            raw_text = self.clean_text(row[i])
            text = raw_text.strip()
            
            # 1. å…ˆåˆ¤æ–­è¡¨å¤´ï¼Œç¡®å®šè¿™æ˜¯å¦æ˜¯â€œçŠ¶æ€/è¿›åº¦â€åˆ—
            h1 = self.clean_text(header1[i] if i < len(header1) else "").strip()
            h2 = self.clean_text(header2[i] if i < len(header2) else "").strip()
            full_header = h1 + h2
            
            # å¿…é¡»åŒ…å«çŠ¶æ€æˆ–è¿›åº¦ï¼Œä¸”ä¸æ˜¯åé¦ˆåˆ—
            if ("çŠ¶æ€" not in full_header and "è¿›åº¦" not in full_header): continue
            if "åé¦ˆ" in full_header: continue

            stage_name = self.find_stage_name_dynamic(i, header1, header2)

            # 2. æ£€æŸ¥ç©ºç™½ (æ–°å¢é€»è¾‘)
            if not text:
                # å¦‚æœæ˜¯ç©ºç™½ï¼Œæ ‡è®°ä¸º missing
                issues.append(("[ç©º]", i, stage_name, 'missing', 0))
                continue
            
            # 3. å¦‚æœä¸æ˜¯ç©ºç™½ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å®‰å…¨è¯ï¼ˆå®Œæˆ/Passç­‰ï¼‰
            if self.is_safe_content(text): continue
            
            # 4. æ£€æŸ¥æ—¥æœŸ (æ—§é€»è¾‘)
            match = re.search(r'(0[1-9]|1[0-2]|[1-9])[\.\-\/]?([0-2][0-9]|3[01]|[1-9])', text)
            if match:
                try:
                    m_str, d_str = match.group(1), match.group(2)
                    month, day = int(m_str), int(d_str)
                    if month > 12 or day > 31: continue

                    year = now.year
                    if now.month == 12 and month == 1: year += 1
                    elif now.month == 1 and month == 12: year -= 1
                    
                    target_date = datetime(year, month, day)
                    days_diff = (now.date() - target_date.date()).days
                    
                    is_today = (days_diff == 0)
                    is_yesterday = (days_diff == 1)
                    
                    if is_today or is_yesterday:
                        issues.append((text, i, stage_name, 'recent', days_diff)) 
                    elif days_diff > 1:
                        issues.append((text, i, stage_name, 'severe', days_diff)) 
                except ValueError: continue
                
        return issues

    def process_single_sheet(self, current_sheet_id):
        sheet_name = self.sheet_names.get(current_sheet_id, f"è¡¨æ ¼({current_sheet_id})")
        print(f"\nğŸ” æ‰«æ [{sheet_name}] ...")
        
        url = f"https://open.feishu.cn/open-apis/sheets/v2/spreadsheets/{SPREADSHEET_TOKEN}/values/{current_sheet_id}!A1:AZ800"
        headers = {"Authorization": f"Bearer {self.token}"}
        resp = requests.get(url, headers=headers).json()
        
        if resp.get("code") != 0: return
        rows = resp.get("data", {}).get("valueRange", {}).get("values", [])
        if not rows or len(rows) < 2: return 
        
        self.scanned_list.append(sheet_name)
        header1 = rows[0]
        header2 = rows[1] if len(rows) > 1 else []
        
        utc_now = datetime.utcnow()
        beijing_now = utc_now + timedelta(hours=8)
        now = beijing_now 
        
        recent_groups = defaultdict(list)
        backlog_groups = defaultdict(list)
        missing_groups = defaultdict(list) # âœ… æ–°å¢ï¼šç©ºç™½é¡¹åˆ†ç»„

        data_rows = rows[2:] if len(rows) > 2 else []

        for i, row in enumerate(data_rows):
            real_row_num = i + 3
            display_name = self.find_shot_number(row)
            
            is_unknown = False
            if not display_name:
                line_content = "".join([self.clean_text(c) for c in row]).strip()
                if not line_content: continue 
                display_name = "æœªçŸ¥ä»»åŠ¡"
                is_unknown = True
            
            row_issues = self.scan_row_full(row, now, header1, header2)
            if not row_issues: continue

            for status_text, col_idx, stage_name, issue_type, days in row_issues:
                # åæ ‡å°¾å·´ï¼šä»…åœ¨æœªçŸ¥æ—¶æ˜¾ç¤º
                coord_info = ""
                if is_unknown:
                    col_char = self.get_column_letter(col_idx)
                    coord_info = f" ({col_char}{real_row_num})"
                
                # âœ… ä¸åŒçš„ issue_type ä½¿ç”¨ä¸åŒçš„æ ¼å¼
                if issue_type == 'missing':
                    self.error_count += 1
                    display_text = f"**[{stage_name}] {display_name}**: æœªå¡«å†™{coord_info}"
                    missing_groups[stage_name].append(f"âšªï¸ {display_text}")

                elif issue_type == 'recent':
                    self.error_count += 1
                    display_text = f"**[{stage_name}] {display_name}**: {status_text}{coord_info}"
                    recent_groups[stage_name].append(f"ğŸŸ  {display_text} (è¿‘æœŸå˜åŠ¨)")

                elif issue_type == 'severe':
                    self.error_count += 1
                    display_text = f"**[{stage_name}] {display_name}**: {status_text}{coord_info}"
                    backlog_groups[stage_name].append(f"ğŸ”´ {display_text} (è¶…æœŸ{days}å¤©)")

        # ç»„è£…æ¶ˆæ¯åˆ—è¡¨ (å«ç©ºè¡Œ)
        final_msg_list = []
        
        # 1. ä¼˜å…ˆå¤„ç†
        if recent_groups:
            final_msg_list.append("âš¡ **ä»Šæ—¥/æ˜¨æ—¥æœ€æ–°å˜åŠ¨ï¼š**")
            for stage, items in recent_groups.items():
                final_msg_list.extend(items)
                final_msg_list.append("") 
            final_msg_list.append("----------------------------------") 
        
        # 2. çŠ¶æ€ç¼ºå¤± (æ–°å¢æ¿å—)
        if missing_groups:
            final_msg_list.append("âš ï¸ **çŠ¶æ€ç¼ºå¤± (æœªå¡«å†™)ï¼š**")
            for stage, items in missing_groups.items():
                final_msg_list.extend(items)
                final_msg_list.append("")
            final_msg_list.append("----------------------------------")

        # 3. å†å²ç§¯å‹
        if backlog_groups:
            final_msg_list.append("ğŸ“‰ **å†å²ç§¯å‹ä¸å¼‚å¸¸é£é™©ï¼š**")
            for stage, items in backlog_groups.items():
                final_msg_list.extend(items)
                final_msg_list.append("") 

        # å‘é€ä¼å¾®é€šé“
        self.send_wecom_alert(sheet_name, final_msg_list, current_sheet_id)

    # ğŸš€ ä¼å¾®å‘é€å‡½æ•°
    def send_wecom_alert(self, sheet_name, msgs, sheet_id):
        if not msgs: return
        valid_lines = [m for m in msgs if m and m.strip()]
        if len(valid_lines) <= 2: return 

        print(f"ğŸš€ å‘é€ä¼å¾®: {sheet_name}")
        
        # ä¼å¾®æ¶ˆæ¯åˆ†ç‰‡
        CHUNK_SIZE = 20
        for i in range(0, len(msgs), CHUNK_SIZE):
            chunk = msgs[i : i + CHUNK_SIZE]
            content_str = "\n".join(chunk)
            
            title = f"## ğŸš¨ è¿›åº¦å¼‚å¸¸æ—¥æŠ¥ | {sheet_name}\n" if i == 0 else ""
            
            footer = ""
            if (i + CHUNK_SIZE) >= len(msgs):
                sheet_url = f"https://feishu.cn/sheets/{SPREADSHEET_TOKEN}?sheet={sheet_id}"
                footer = f"\n\n> ğŸ”— [ç‚¹å‡»è¿›å…¥é£ä¹¦è¡¨æ ¼]({sheet_url})"

            payload = {
                "msgtype": "markdown",
                "markdown": {
                    "content": f"{title}{content_str}{footer}"
                }
            }
            try:
                requests.post(WECOM_WEBHOOK, json=payload)
                time.sleep(0.5)
            except: pass

    def send_summary(self):
        print("å‘é€æ±‡æ€»...")
        wc_content = f"## âœ… å·¡æ£€å®Œæˆæ—¥æŠ¥\n**å…±æ‰«æ {len(self.scanned_list)} ä¸ªè¡¨æ ¼**\nğŸš« **å‘ç°é£é™©é¡¹ï¼š** <font color=\"warning\">{self.error_count}</font> ä¸ª"
        try: 
            time.sleep(0.5)
            requests.post(WECOM_WEBHOOK, json={"msgtype": "markdown", "markdown": {"content": wc_content}})
        except: pass

    def run(self):
        print("ğŸ¤– V49.3 (Status Missing Detection)...")
        self.load_all_sheet_names()
        for sheet_id in TARGET_SHEET_IDS:
            try:
                self.process_single_sheet(sheet_id)
                time.sleep(1)
            except Exception as e:
                print(f"âš ï¸ æ‰«æå‡ºé”™ [{sheet_id}]: {e}")
        self.send_summary()
        print("âœ… ä»»åŠ¡å…¨éƒ¨ç»“æŸ")

if __name__ == "__main__":
    MonitorBot().run()
